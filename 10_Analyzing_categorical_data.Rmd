---
title: "10 Analyzing categorical data"
author: "Dr Juan H Klopper"
output:
  html_document:
    toc: true
    number_sections: false
---

<style type="text/css">
h1 {color:#1a2451;}
h2 {color:#ffbd4a;}
h3 {color:#1a2451;}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd(getwd()) # Spreadsheet file and R markdown file in same folder
```

```{r Libraries, message=FALSE, warning=FALSE}
library(dplyr)
```

![](KRG_elegant_logo_for_light_BG.png)

## Preamble

Many research projects contain categorical, and more specifically, nominal categorical variables.  The sample space of these variables are not numbers, yet we require some way of analyzing them.

To the rescue comes the tests for categorical variables where we use the counts of the sample space elements to answer our research questions.

In this chapter we will meet the common tests that are used to answer these questions.  Take careful note of what their results say about the data, so as to use the correct test for your data.

## The $\chi^2$ goodness of fit test

The $chi^2$ goodness of fit test examines the proportions of the sample space data point values for a nominal categorical variable.  It is a test of proportions.  Below, we import the `ProjectDataII.csv` file and save it as a data frame object with the computer variable name `df`.

```{r Data import}
df <- read.csv("ProjectIIData.csv")
```

One of the variables in the dataset is `Improvement`.  In this simulated dataset, it refers to whether patients improved after receiving treatment.  The sample space is dichotomous, with values `Yes` and `No`.  We can use the `table()` function to see a count for each of these elements.

```{r Count of the sample space elements for the improvement variable}
table(df$Improvement)
```

A total of $156$ patients did not improve, whereas $144$ did.  Before starting the data collection, we might have suspected a $0.5:0.5$ split in the data.  Instead, we found proportions of `r 156 / (156 + 144)`:`r 144 / (156 + 144)`.  We can test our findings against the expected proportions using the `chisq.test()` function.  The first argument is a numeric vector containing the actual counts.  The `p =` argument holds a numerical vector containing the expected proportions, which must sum to $1.0$.

```{r Chi square goodness of fit test}
chisq.test(c(156, 144),
           p = c(0.5, 0.5))
```

Since we only have a sample space of two elements, we have a single degree of freedom.  For the appropriate $chi^2$ distribution, our findings represent a _p_ value of $0.49$.  For an $\alpha$ value of $0.05$, we cannot reject the null hypothesis that there is no significant difference between the observed and expected proportions.

## The $chi^2$ test for independence

The $chi^2$ test for independence measures, as the name implies, whether two categorical variables are independent.  In our dataset, we also have a categorical variable named `Group`.  We can inspected it with the `table()` function too.

```{r Sample space elements count for the Group variable}
table(df$Group)
```

Here we see that there are $150$ patients in each group.  We can combine this information with that of the `Improvement` variable into what is termed a _contingency_ table of observed values.  We use the same `table()` function, but list both variables.

```{r Contingency table of Group vs Improvement}
table(df$Group,
      df$Improvement)
```

Now we can see that $66$ patients in group A did not improve, while $84$ did.  For patients in group C, these numbers are $90$ and $60$.  We can use this contingency table of observed values to investigate whether the two nominal categorical variables are _independent_ of each other.

```{r Chi square test for independence}
chisq.test(table(df$Group,
                 df$Improvement),
           correct = FALSE)  # Ignoring Yates' correction for continuity
```

The degrees of freedom for this contingency table is $1$.  This is calculated as $1$ subtracted from the number of rows times $1$ subtracted from the number of columns, i.e $\left( 2 - 1 \right) \times \left( 2 - 1 \right) = 1 \times 1  = 1 $.  The $chi^2$ value is $7.6923$ representing a _p_ value of < $0.01$.  Note that given an $\alpha$ value of $0.05$, we do not specify values smaller than $0.01$ when reporting the _p_ value.  It is simply stated as being less than $0.01$.  Given this small _p_ value, we can reject our null hypothesis that the variables are not dependent and accept the alternative hypothesis that they are indeed dependent.  Which group the patient ended up in, did influence their _improvement_ outcome.

The $chi^2$ test works on the principle of subtracting an expected table, $E$, from our table of observed values, $O$.  This is shown in equation (1).

$$ \chi^2 = \frac{\sum_{i=1}^{n} \left( \text{O}_i - \text{E}_i \right)^2}{\text{E}_i} \tag{1}$$

This simulated study had a total of $300$ subjects.  Some $150$ were in group A and $150$ in group B.  Conversely, $156$ improved and $144$ did not.  Since the observed table is $ 2 \times 2 $ in size, the expected table will be as well.  Each value in the latter is determined by its position.  For the value in row $1$, column $1$, we multiply the sum of row $1$ and column $1$ in the observed table and divide it by the sample total.  This would be $156 \times 150 \div 300 = 78$.  The same goes for the other three values.  The _p_ value is calculated from the $\chi^2$ distribution for the given degrees of freedom.

We need not confine ourselves to $2 \times 2 $ contingency tables.  If each nominal categorical variable has more than two elements in their sample spaces, say $n$ and $m$, we would create a $n \times m $ table.

Note that the $\chi^2$ distribution is a continuous distribution, yet our nominal categorical variable is, in essence, a discrete variable.  Yates' correction for continuity is sometimes used to solve this _problem_ of possible over correction.  It aims at correcting this _problem_ created by assuming that the discrete probabilities of counts in the table can be approximated by a continuous distribution.  It subtracts a half, ($0.5$), from each subtraction of the expected from the observed table.  It is shown in equation (2), but is not commonly used.

$$ \chi^2 = \frac{\sum_{i=1}^{n} \left( |  \text{O}_i - \text{E}_i | - 0.5 \right)^2}{\text{E}_i} \tag{2}$$

One problem that arises from this approximation of a discrete frequency distribution, and that we must consider, is that of a small sample size.  As a rule of thumb, we run into this problem with the $\chi^2$ test for independence if $80$% or more of the table entries have less than five samples.  In this case, we make use of Fisher's exact test.

## Fisher's exact test

Fisher's test is termed an exact test, as the _p_ value is calculated directly.  It does not follow from a sampling distribution.  This test also requires the creation of a contingency table, but it is limited to a $ 2 \times 2 $ table.  If there are more than two elements in the sample space of either of the categorical variables, then they have to be combined in a way that makes sense fr=or the research question at hand.

The equation for Fisher's exact test for a total sample size of $N$, is shown in equation (3).

$$ \text{contingency table} = \begin{pmatrix} a & b \\ c & d \end{pmatrix} \\ p = \frac{\left( a + b \right)! \left( c + d  \right)! \left( a + c \right)! \left(  \right)!b + d }{a! b! c! d! N!} \tag{3}$$

The `fisher.test()` function uses equation (3).  The values have to be entered as a matrix, which we create below.  Note that we list all four values, but then specify that we require two rows.  This will neatly split the data after the first two values.  Be sure to enter the values in to correct order when copying from your data.

```{r Creating a contingency table with small sample counts}
contingency <- matrix(c(3, 4, 6, 2),  # The fisher.test function requires a matrix as imput
                      nrow = 2)  # Creating two rows from the four elements
contingency
```

```{r Fisher exact test}
fisher.test(contingency)
```

We note a _p_ value of $0.31$.

## McNemar test

This test is used for matched pairs of subjects.  Two categorical variables are used and data point values are captured for both in each subject.  In practice, these are either _before_ and _after_ variables or homozygotic twins, and so on.

Both variables require a dichotomous outcome, i.e. the sample space only contains two elements.  This results in a $ 2 \times 2 $ contingency table.  The equation results in a $\chi^2$ value with a single degree of freedom.  This is shown in equation (4).

$$ \chi^2 =  \frac{{\left( b - c \right)}^2}{b + c} \tag{4} $$

Below, we have an example of $314$ patients receiving a treatment.  Before $101$ patients had a certain symptom and $59$ did not.  After the treatment $121$ had the symptoms and $34$ did not.

```{r Creating a contingency stable for the McNemar test}
contingency <- matrix(c(101, 59, 121, 34),
                      nrow = 2,
                      byrow = FALSE)
contingency
```

Note how, in our contingency table, the before values run across the rows and that the after values run across the columns with  the positive outcome listed first and the negative outcome second.

The `mcnemar.test()` function calculates the $\chi^2$ and _p_ value for us, based on a single degree of freedom.

```{r Calculating a p value for the McNemar test}
mcnemar.test(contingency,
             correct = FALSE)  # Ignoring continuity correction
```

Note that we once again ignore continuity correction.

## Conclusion

The various tests for categorical variables are very commonly used in the literature.  Make sure to use the correct test based on the research question and the sample size.