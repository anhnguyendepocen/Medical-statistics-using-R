---
title: "Non-parametric tests"
author: "Dr Juan H Klopper"
output:
  html_document:
    toc: true
    number_sections: false
---

<style type="text/css">
h1 {color:#1a2451;}
h2 {color:#ffbd4a;}
h3 {color:#1a2451;}
</style>

![](KRG_elegant_logo_for_light_BG.png)

## Preamble

In the previous chapter we learned that we cannot always use the common parametric tests to analyze our data.

This chapter is all about the proper alternatives.  These non-parametric tests are not dependent upon the parameters that we saw in the assumptions for the use of parametric tests.

## The Wilcoxon rank-sum test

The Wilcoxon rank-sum test is used in place of the unpaired sample _t_ test (Student's _t_ test).  In this situation, the collected data point values do not meet the assumptions for the use of a parametric test.  It is, very confusingly, also known as the _Mann-Whitney U test_, the _Wilcoxon-Mann-Whitney test_ and the _Mann-Whitney-Wilcoxon test_. 

Although the parametric assumptions are not required, this test does require that the data point values are ordinal in some way, i.e. that they can be ranked in order.  It makes this test ideal for the use of ordinal categorical data point values expressed as numbers too.

All the data point values from the two groups are placed in the same set and ranked in ascending order.  The lowest value is assigned a rank of $1$ and so on, until the largest value, given the last (and highest) rank.

The groups are again separated and the ranks in each grouped summed.  The null hypothesis is that these sums will be even in both groups.  This means that it is equally likely that a randomly selected sample selected from one group will be less than or greater than a randomly selected sample from the other group.

Measurement capabilities and rounding do effect the Wilcoxon rank-sum test.  If measurements are only accurate to a certain decimal value, two data point values will be given the same rank, even though they might not be equal given a more accurate measurement (more decimal values).  Rounding of numerical values have a similar effect.

To investigate this test, we will create some simulated data and save it as a data frame object.

### Creating data

Below we create a data frame object with two statistical variables, `Group` and `CRP`.  The former is nominal categorical with a sample space containing two elements.  We will use this variable to construct two groups.  The latter variable is numerical and consists of $100$ values following a $\chi^2$ distribution with a degrees of freedom value of $2$.

```{r Creating data and a data.frame}
set.seed(123)  # Seeding the pseudo-random number generator for reproducible results
group <- sample(c("A", "B"),
                100,
                replace = TRUE)

df <- data.frame(Group = group,
                 CRP = round(rchisq(100,  # Non-normal distribution
                                    2),
                             digits = 1))
```

We can view this first six rows using the `head()` function.
 
```{r Firt six rows}
head(df)

```

### Conducting the test

The `wilcox.test()` function will conduct our Mann-Whitney-U tests.  We use formula notation and separately define the data frame object using the `data = ` keyword argument.  Note that we could also have used the `df$CRP ~ df$Group` syntax, omitting the second argument.

```{r Wilcoxon rank sum test}
test1 <- wilcox.test(CRP ~ Group,
                     data = df)
```

Let's look at the results of our test.

```{r Results of the Wilcoxon rank-sum tests}
test1
```

We note a test statistic and a _p_ value, which we can use in a similar fashion as the Student's _t_ test in our reports.

As and aside, the `wilcox.test()` function also allows the use of data stored in two separate lists.

```{r Alternative Wilcoxon rank-sum test format}
grpA <- rnorm(50,
              0,
              1)
grpB <- rchisq(50,
               1)

wilcox.test(grpA,
            grpB)
```

### Arguments

It is worthwhile to take a closer look at the arguments of this function.

* `paired =`
  * This can be set as `FALSE` (default), if the groups are independent, or `TRUE` if they are dependent
* `alternative = c()`
  * Any one of `two.sided`, `less`,or `greater` depending on the alternative hypothesis
* `mu = `
  * Sets the expected difference in medians
* `exact =`
  * Performs an exact test
  * Default is TRUE if total sample size less than $50$ and there are no ties
  * Force the use of normal distribution approximation by setting to FALSE
* `correct =`
  * Either `TRUE` or `FALSE` (default is FALSE), with in the case of the former, applies continuity correction to the calculation of the _p_ value
* `conf.int =`
  * If set to TRUE, calculates the 95% confidence intervals of the median of all the differences between data point values for the two groups
  * Also give the difference in the current case
  * Can be approximated using bootstrapping
* `conf.level =`
  * Sets the confidence level
* `na.action = `
  * Determines what to do with null values, most commonly being `na.exclude`
  
  
### Effect size of the Wilcoxon sum-rank test

Effect sizes are used more commonly today.  For the Mann-Whitney-U test, this can be calculated approximately, reverting the _p_ value to a _z_ score using the `qnorm()` command.  The _z_ score is then expressed as an _r_ value using equation (5).

$$ r = \frac{z}{\sqrt{n}} \tag{5}  $$

### Example

Let's look at an example of simulated data to see the process of choosing a non-parametric tests in action.

Given two groups, `Treatment` and `Control`, note the two sets of data point values for the variable `LDL`

```{r Data for two groups}
treatment <- c(1.8, 1.7, 6.9, 0.8, 2.1, 7.6)
contrl <- c(4.1, 3.2, 6.8, 1.9, 3.0, 4.7)
```

A Shapiro-Wilk test shows that the first group is not from a normal distribution.

```{r Shapiro-Wilk test}
shapiro.test(treatment)
```

To make life easier, we create a data frame object from our two lists.

Creating a `data.frame`

```{r Creating a data.frame}
df <- data.frame(LDL = c(treatment, contrl),
                 Group = rep(c("Treatment", "Control"),
                             each = 6))
head(df)
```

Displaying a simple box plot gives us a great visual indicator of the data.

```{r Box plot}
boxplot(LDL ~ Group,
        df,
        main = "LDL for Control and Treatment groups",
        xlab = "Groups",
        ylab = "LDL")
```

The skewness in the data (especially for the treatment group) is clearly visible.

Now for conducting the Wilcoxon rank-sum test.

```{r Wilcoxon rank-sum test}
wilcox.test(LDL ~ Group,
            data = df)
```

The $W$ statistic is the number of times that a data point value from the Treatment group is lower than that of the Control group.

Sorting the data point values makes this easier to understand.

```{r Sorting}
treatment <- sort(treatment)
contrl <- sort(contrl)
treatment
contrl
```


## Wilcoxon signed-rank test

This is the non-parametric equivalent to the _paired-sample t test_.

The same `wilcox.test()` function is used for this test, except that the keyword argument `paired =` is set to `TRUE`.

## The Kruskal-Wallis test

The _Kruskal-Wallis test_ is the non-parametric equivalent of analysis of variance.  Here we are comparing more than two groups.

Below we create data for a placebo group and for two treatment groups, let's say a low-dose and a high-dose group.

```{r Data for three groups}
treatment_low <- c(1.8, 1.7, 6.9, 0.8, 2.1, 7.6)
treatment_hi <- c(3, 4.2, 7, 4.5, 7.8, 3.3, 7.8)
contrl <- c(4.1, 3.2, 6.8, 1.9, 3.0, 4.7)
```

The `kruskal.test()` function takes a `list()` function as argument.  The three vectors of data point values are specified.

```{r Conduction the Kruskal Wallis test}
kruskal.test(list(treatment_low,
                  treatment_hi,
                  contrl))
```

We note a Kruskal-Wallis $\chi^2$ statistic, degrees of freedom, and a _p_ value.

## Non-parametric alternatives to correlation

The assumptions for the use of parametric tests applies to correlation.  Both variables must meet these assumptions.

The first alternative ranks the values before calculating the correlation and is termed *Spearman correlation* and the resultant correlation coefficient is termed *Spearman's* $\rho$.  This can be passed as an argument to the `cor.test()` command.

First, we create some paired data point values for our correlation.

```{r Creating variables for nonparametric correlation}
height <- c(160, 167, 170, 168, 172, 159, 163, 155, 158, 180, 185, 181, 195)
weight <- height + (round((rnorm(length(height), 0, 1)), 3) * 10)
```

A scatter plot shows the pairs of values.

```{r Scatter plot}
plot(height,
     weight,
     main = "Correlation between height and weight",
     xlab = "Height",
     ylab = "Weight",
     las = 1)
```

Now for the Spearman correlation test, setting the `method = ` argument for the `cor.test()` function to `"Spearman"`.

```{r Spearman correlation test}
cor.test(height,
         weight,
         method = "spearman")
```

The correlation coefficient in this case is $\rho$.  One problem with Spearman's $\rho$ is that it deals poorly with small sample sizes and a large number of tied ranks.  *Kendall's* $\tau$ is an alternative non-parametric test for these cases.

```{r}
cor.test(height,
         weight,
         method = "kendall")
```

## Conclusion

These are the non-parametric test alternatives for the commonly used parametric tests.  Always check that the assumptions for the use of parametric tests are met.  If not, do use these non-parametric alternatives.